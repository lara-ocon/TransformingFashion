{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate CLIP Embeddings for Polyvore Dataset\n",
        "\n",
        "This notebook replicates the embedding generation process from the *OutfitTransformer* repository ([owj0421/outfit-transformer](https://github.com/owj0421/outfit-transformer)), specifically the script `src/run/1_generate_clip_embeddings.py`. It generates CLIP embeddings for the Polyvore dataset using the `patrickjohncyh/fashion-clip` model, saving them in the same format (`polyvore_{rank}.pkl`) and location (`{polyvore_dir}/precomputed_clip_embeddings`). The goal is to produce embeddings identical to the official implementation, which we can verify by comparing with the official outputs.\n",
        "\n",
        "## Setup\n",
        "- **Dataset**: Polyvore (~251,008 items, `item_metadata.json`, `images/`).\n",
        "- **Model**: `patrickjohncyh/fashion-clip` (CLIP ViT-B/32).\n",
        "- **Output**: Embeddings saved as `{polyvore_dir}/precomputed_clip_embeddings/polyvore_{rank}.pkl`.\n",
        "- **Environment**: Python 3.8+, PyTorch 1.9.0, transformers, numpy, etc.\n",
        "\n",
        "## Prerequisites\n",
        "Run the following to install dependencies:\n",
        "```bash\n",
        "pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "pip install transformers==4.9.2 numpy pickle5 tqdm wandb pillow\n",
        "pip install git+https://github.com/patrickjohncyh/fashion-clip.git\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ia/Documentos/TFG_Lara/TransformingFashion/outfit_transformer_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import pickle\n",
        "\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Define paths\n",
        "SRC_DIR = pathlib.Path(os.getcwd()).absolute()\n",
        "LOGS_DIR = SRC_DIR / 'logs'\n",
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "POLYVORE_DIR = './datasets/polyvore'  # Adjust if your dataset is elsewhere\n",
        "POLYVORE_PRECOMPUTED_CLIP_EMBEDDING_DIR = f\"{POLYVORE_DIR}/precomputed_clip_embeddings\"\n",
        "POLYVORE_METADATA_PATH = f\"{POLYVORE_DIR}/item_metadata.json\"\n",
        "POLYVORE_IMAGE_DATA_PATH = f\"{POLYVORE_DIR}/images/{{item_id}}.jpg\"\n",
        "\n",
        "# Configure logging\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=LOGS_DIR / 'precompute_clip_embedding.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('precompute_clip_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Utility Functions\n",
        "\n",
        "We replicate the dataset loading, model setup, and distributed processing logic from the official implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for utility functions\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# Define FashionItem class (from datatypes.py)\n",
        "class FashionItem:\n",
        "    def __init__(self, item_id, category, image=None, description=\"\", metadata=None, embedding=None):\n",
        "        self.item_id = item_id\n",
        "        self.category = category\n",
        "        self.image = image\n",
        "        self.description = description\n",
        "        self.metadata = metadata or {}\n",
        "        self.embedding = embedding\n",
        "\n",
        "# Load metadata (from polyvore.py)\n",
        "def load_metadata(dataset_dir):\n",
        "    metadata = {}\n",
        "    with open(POLYVORE_METADATA_PATH.format(dataset_dir=dataset_dir), 'r') as f:\n",
        "        metadata_ = json.load(f)\n",
        "        for item in metadata_:\n",
        "            metadata[item['item_id']] = item\n",
        "    logger.info(f\"Loaded {len(metadata)} metadata\")\n",
        "    print(f\"Loaded {len(metadata)} metadata\")\n",
        "    return metadata\n",
        "\n",
        "# Load image (from polyvore.py)\n",
        "def load_image(dataset_dir, item_id, size=(224, 224)):\n",
        "    image_path = POLYVORE_IMAGE_DATA_PATH.format(dataset_dir=dataset_dir, item_id=item_id)\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading image {image_path}: {e}\")\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load item (from polyvore.py)\n",
        "def load_item(dataset_dir, metadata, item_id, should_load_image=False, embedding_dict=None):\n",
        "    metadata_ = metadata[item_id]\n",
        "    return FashionItem(\n",
        "        item_id=metadata_['item_id'],\n",
        "        category=metadata_['semantic_category'],\n",
        "        image=load_image(dataset_dir, metadata_['item_id']) if should_load_image else None,\n",
        "        description=metadata_['title'] if metadata_.get('title') else metadata_['url_name'],\n",
        "        metadata=metadata_,\n",
        "        embedding=embedding_dict[item_id] if embedding_dict else None\n",
        "    )\n",
        "\n",
        "# PolyvoreItemDataset (from polyvore.py)\n",
        "class PolyvoreItemDataset:\n",
        "    def __init__(self, dataset_dir, metadata=None, embedding_dict=None, load_image=False):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.metadata = metadata if metadata else load_metadata(dataset_dir)\n",
        "        self.load_image = load_image\n",
        "        self.embedding_dict = embedding_dict\n",
        "        self.all_item_ids = list(self.metadata.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_item_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return load_item(self.dataset_dir, self.metadata, self.all_item_ids[idx], \n",
        "                         should_load_image=self.load_image, embedding_dict=self.embedding_dict)\n",
        "\n",
        "# Collate function (from collate_fn.py)\n",
        "def item_collate_fn(batch):\n",
        "    return [item for item in batch]\n",
        "\n",
        "# Distributed setup (from utils/distributed_utils.py)\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "# Seed everything (from utils/utils.py)\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Arguments\n",
        "\n",
        "We replicate the argument parsing from `1_generate_clip_embeddings.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments: {}\n"
          ]
        }
      ],
      "source": [
        "# Define arguments (from parse_args)\n",
        "class Args:\n",
        "    model_type = 'clip'\n",
        "    polyvore_dir = './datasets/polyvore'\n",
        "    polyvore_type = 'nondisjoint'\n",
        "    batch_sz_per_gpu = 128\n",
        "    n_workers_per_gpu = 4\n",
        "    checkpoint = None\n",
        "    world_size = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "    demo = False\n",
        "\n",
        "args = Args()\n",
        "print(f\"Arguments: {vars(args)}\")\n",
        "\n",
        "# Set seed\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model Loading\n",
        "\n",
        "We replicate the model loading logic from `models/load.py`, using `patrickjohncyh/fashion-clip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model loading (simplified from models/load.py)\n",
        "def load_model(model_type='clip', checkpoint=None):\n",
        "    if model_type != 'clip':\n",
        "        raise ValueError(\"Only 'clip' model_type is supported in this notebook\")\n",
        "    \n",
        "    model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
        "    processor = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
        "    model.eval()\n",
        "    \n",
        "    if checkpoint:\n",
        "        state_dict = torch.load(checkpoint, map_location='cpu')\n",
        "        model.load_state_dict(state_dict)\n",
        "    \n",
        "    return model, processor\n",
        "\n",
        "# Precompute CLIP embedding function\n",
        "def precompute_clip_embedding(model, processor, batch):\n",
        "    images = [item.image for item in batch]\n",
        "    texts = [item.description for item in batch]\n",
        "    \n",
        "    # Process images\n",
        "    inputs = processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds  # (batch_size, 512)\n",
        "        text_embeds = outputs.text_embeds   # (batch_size, 512)\n",
        "        embeddings = torch.cat((image_embeds, text_embeds), dim=-1)  # (batch_size, 1024)\n",
        "    \n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Embeddings\n",
        "\n",
        "We replicate the `compute` function from `1_generate_clip_embeddings.py`, adapting it for DDP in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 251008 metadata\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rank 0: 100%|██████████| 1961/1961 [12:47<00:00,  2.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 0: Computed 251008 embeddings\n",
            "Rank 0: Saved embeddings to ./datasets/polyvore/precomputed_clip_embeddings/polyvore_0.pkl\n"
          ]
        }
      ],
      "source": [
        "def compute(rank, world_size, args):\n",
        "    # Setup DDP\n",
        "    setup(rank, world_size)\n",
        "    logger.info(f\"Logger Setup Completed\", extra={'rank': rank})\n",
        "    \n",
        "    # Setup dataloader\n",
        "    item_dataset = PolyvoreItemDataset(\n",
        "        dataset_dir=args.polyvore_dir,\n",
        "        load_image=True\n",
        "    )\n",
        "    \n",
        "    n_items = len(item_dataset)\n",
        "    n_items_per_gpu = n_items // world_size\n",
        "    start_idx = n_items_per_gpu * rank\n",
        "    end_idx = start_idx + n_items_per_gpu if rank < world_size - 1 else n_items\n",
        "    item_dataset = torch.utils.data.Subset(item_dataset, range(start_idx, end_idx))\n",
        "    \n",
        "    item_dataloader = DataLoader(\n",
        "        dataset=item_dataset,\n",
        "        batch_size=args.batch_sz_per_gpu,\n",
        "        shuffle=False,\n",
        "        num_workers=args.n_workers_per_gpu,\n",
        "        collate_fn=item_collate_fn\n",
        "    )\n",
        "    logger.info(f\"Dataloaders Setup Completed\", extra={'rank': rank})\n",
        "    \n",
        "    # Load model\n",
        "    model, processor = load_model(model_type=args.model_type, checkpoint=args.checkpoint)\n",
        "    model.to(rank)\n",
        "    if world_size > 1:\n",
        "        model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "    logger.info(f\"Model Loaded\", extra={'rank': rank})\n",
        "    \n",
        "    # Compute embeddings\n",
        "    all_ids, all_embeddings = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(item_dataloader, desc=f\"Rank {rank}\"):\n",
        "            if args.demo and len(all_embeddings) > 10:\n",
        "                break\n",
        "            \n",
        "            embeddings = precompute_clip_embedding(model.module if world_size > 1 else model, processor, batch)\n",
        "            all_ids.extend([item.item_id for item in batch])\n",
        "            all_embeddings.append(embeddings)\n",
        "    \n",
        "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "    logger.info(f\"Computed {len(all_embeddings)} embeddings\", extra={'rank': rank})\n",
        "    print(f\"Rank {rank}: Computed {len(all_embeddings)} embeddings\")\n",
        "    \n",
        "    # Save embeddings\n",
        "    save_dir = POLYVORE_PRECOMPUTED_CLIP_EMBEDDING_DIR.format(polyvore_dir=args.polyvore_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = f\"{save_dir}/polyvore_{rank}.pkl\"\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump({'ids': all_ids, 'embeddings': all_embeddings}, f)\n",
        "    logger.info(f\"Saved embeddings to {save_path}\", extra={'rank': rank})\n",
        "    print(f\"Rank {rank}: Saved embeddings to {save_path}\")\n",
        "    \n",
        "    # Cleanup DDP\n",
        "    cleanup()\n",
        "\n",
        "# Run computation\n",
        "world_size = args.world_size\n",
        "if world_size > 1:\n",
        "    mp.spawn(\n",
        "        compute,\n",
        "        args=(world_size, args),\n",
        "        nprocs=world_size,\n",
        "        join=True\n",
        "    )\n",
        "else:\n",
        "    compute(0, 1, args)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "outfit_transformer_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
